@CONTEXT/1.2 profile=human canon=CTX-CANON/3
[ns]
ctx=ctx:
ops=ops:

[meta]
doc=01jdzr7v95kq7b9n91hza7ap8d
author=context-team
date=2025-10-28T18:00:00Z
schema=CONTEXT
version=1.2
units=ctx.tokens
lang=en
policy.cf=Confidence derived from A/B evaluation (success ratio).
policy.ttl=Revise capsules after TTL or earlier if superseded.

[cap i_token_saving]
t=ctx.I
p=9
cf=0.830
ts=2025-10-28T18:00:00Z
ttl=P60D
src=exp:ab-42
lang=en
tags=insight,latency,token_budget
d=Explicit capsules with relations reduce prompt length by ~20% on multi-hop tasks.

[cap k_learning_rate]
t=ctx.K
p=7
cf=0.920
ts=2025-10-28T17:58:00Z
ttl=P180D
src=paper:adamw
lang=en
tags=ml,optimization,training
d=Reducing the learning rate after plateaus improves validation loss by 3-7%.

[cap s_rag_pipeline]
t=ctx.S
p=8
cf=0.900
ts=2025-10-28T17:59:00Z
ttl=P90D
src=ops:rag
lang=en
tags=procedure,rag
note=Procedure tuned for multi-hop QA.
d@text/markdown<<MD
1. Formulate retrieval intent.
2. Retrieve top-k documents (k=6, Î»=0.3).
3. Compress snippets to 4-6 bullets.
4. Synthesize answer with attribution.
MD

[cap t_deduce]
t=ctx.T
p=7
cf=0.880
ts=2025-10-28T18:00:06Z
ttl=P30D
src=ops:rag
lang=en
tags=trace
in=k_learning_rate,t_retrieve
out=i_token_saving
op=ctx.deduce
cost.kind=ctx.tokens
cost.val=128
d=Summarized fragments and produced the token-saving insight.

[cap t_retrieve]
t=ctx.T
p=7
cf=0.900
ts=2025-10-28T18:00:05Z
ttl=P30D
src=ops:rag
lang=en
tags=trace
in=s_rag_pipeline
out=t_deduce
op=ctx.retrieve
cost.kind=ctx.tokens
cost.val=384
d=Fetched 6 relevant fragments.

[trace ch_prompt_efficiency]
goal=i_token_saving
head=t_retrieve
halt=t_deduce
status=ctx.ok
ts=2025-10-28T18:00:10Z
tags=prompt-efficiency,reasoning

[rel]
r=k_learning_rate|ctx.supports|i_token_saving|w=0.6|ts=2025-10-28T18:00:15Z
r=s_rag_pipeline|ctx.clarifies|i_token_saving
r=t_deduce|ctx.derived_from|k_learning_rate
r=t_retrieve|ctx.applied_by|t_deduce

[footer]
digest=sha256
digest-base16=c10b26c9342e85a8a99579f9eed74c04805d5b9e605164460b949f6f35928000
